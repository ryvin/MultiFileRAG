# ===== Server Configuration =====
# Host address to bind the server to (0.0.0.0 allows external connections)
HOST=0.0.0.0
# Port number for the server
PORT=9621
# Number of worker processes
WORKERS=2

# ===== Document Indexing Settings =====
# Enable caching of LLM responses during entity extraction (true/false)
ENABLE_LLM_CACHE_FOR_EXTRACT=true
# Language for document summaries (English, Spanish, Chinese, etc.)
SUMMARY_LANGUAGE=English
# Maximum number of documents to process in parallel (lower for large models)
MAX_PARALLEL_INSERT=2

# ===== LLM Configuration =====
# Timeout in seconds for LLM operations (increase for complex documents)
TIMEOUT=600
# Temperature for LLM responses (0.0-1.0, lower for more deterministic responses)
TEMPERATURE=0.1
# Maximum concurrent LLM requests
MAX_ASYNC=4
# Maximum tokens to send to LLM (must be less than model's context window)
MAX_TOKENS=32768

# ===== Ollama LLM Configuration =====
# LLM binding type (ollama, openai, azure_openai, etc.)
LLM_BINDING=ollama
# LLM model to use
# Available models: llama3, llama3:8b, llama3:70b, mistral, mixtral, phi3, gemma, gemma:2b, codellama, deepseek-r1:32b, etc.
LLM_MODEL=deepseek-r1:14b
# Ollama API endpoint
LLM_BINDING_HOST=http://localhost:11434

# ===== Embedding Configuration =====
# Embedding binding type (ollama, openai, azure_openai, etc.)
EMBEDDING_BINDING=ollama
# Embedding API endpoint
EMBEDDING_BINDING_HOST=http://localhost:11434
# Embedding model to use
# Available models: nomic-embed-text, bge-m3, bge-small-en-v1.5, etc.
EMBEDDING_MODEL=bge-m3
# Embedding dimensions (bge-m3 requires 1024, most others use 768)
EMBEDDING_DIM=1024

# ===== Web UI Configuration =====
# Title displayed in the web UI
WEBUI_TITLE=MultiFileRAG
# Description displayed in the web UI
WEBUI_DESCRIPTION=Process and query PDF, CSV, and image files with LightRAG

# ===== Storage Configuration =====
# Key-value storage implementation
LIGHTRAG_KV_STORAGE=PGKVStorage
# Vector database storage implementation
LIGHTRAG_VECTOR_STORAGE=PGVectorStorage
# Knowledge graph storage implementation
LIGHTRAG_GRAPH_STORAGE=Neo4JStorage
# Document status storage implementation
LIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage
# Cache storage implementation (commented out until properly registered)
# LIGHTRAG_CACHE_STORAGE=HybridCacheStorage

# ===== PostgreSQL Configuration =====
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DATABASE=multifilerag
POSTGRES_WORKSPACE=default

# ===== Neo4j Configuration =====
# Neo4j connection parameters
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=multifilerag
NEO4J_DATABASE=neo4j
# Neo4j graph storage parameters
LIGHTRAG_GRAPH_URI=bolt://localhost:7687
LIGHTRAG_GRAPH_USER=neo4j
LIGHTRAG_GRAPH_PASSWORD=multifilerag
LIGHTRAG_GRAPH_DATABASE=neo4j

# ===== Redis Configuration =====
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
REDIS_TTL=3600

# ===== Cache Configuration =====
# Use Redis for caching
LIGHTRAG_CACHE_ENABLED=true
LIGHTRAG_CACHE_TYPE=redis
LIGHTRAG_CACHE_TTL=3600

# ===== Node.js Database Configuration =====
NODEJS_DB_HOST=localhost
NODEJS_DB_PORT=3000
NODEJS_DB_USER=admin
NODEJS_DB_PASSWORD=admin
NODEJS_DB_NAME=multifilerag_nodejs

# ===== Directory Configuration =====
# Directory for storing RAG data (vectors, graphs, etc.)
WORKING_DIR=./rag_storage
# Directory for input documents
INPUT_DIR=./inputs

# ===== Query Configuration =====
# Number of top results to retrieve (increase for more comprehensive answers)
TOP_K=200
# Cosine similarity threshold for vector search (0.0-1.0)
COSINE_THRESHOLD=0.2
# Maximum tokens for text chunks
MAX_TOKEN_TEXT_CHUNK=6000
# Maximum tokens for relation descriptions
MAX_TOKEN_RELATION_DESC=6000
# Maximum tokens for entity descriptions
MAX_TOKEN_ENTITY_DESC=6000
